function output_complete = preprocessing_context_fast_gpu(data_in,dim,frac_scale)
% PREPROCESSING_CONTEXT_FAST_GPU is a runtime optimized preprocessing script 
% to encode input data with a VSA running on GPU (without VSA_toolbox)
%
% This version can exploits that there are data duplicates
% e.g. in the UAH dataset. There, the end (second half 33:64) of each sequence is the beginning 
% (first halt 1:32) of the next sequence. This fundtion vesrion avoids duplicate computations.
%
%
% All operations are vectorized and performed on GPU. It is highly recommended to run 
% on the largest possible chunk of data for maximum (runtime) performance 
% (e.g. Nvidia GTX1080Ti and 2048 dimension: 2000 sample sequences of length 64x9)
%
% The function does a lot of initial allococation and some precomputations. The relevant 
% runtime is measured inside the function!
% 
% data ... single(!) array
% 
% example call:
%
% data = load('data/uah_dataset.mat');
% D = data.motorway_dataset(1:2000,:,:); DU = single(D);
% output=preprocessing_context_fast_duplicateData(DU,2048,6);
% 
%   INPUT: 
%       data_in         -   data array with size of n x t x m (n... number of samples,
%                           t... the number of time-steps and m... the number of
%                           sensortypes 
%       dim             -   number of dimensions of the resulting high-dimensional
%                           vectors
%       frac_scale      -   scaling of fractional binding 
%
%   OUTPUT:
%       output_complete -   output array with size of n x d (d... number of
%                           dimensions and n... the number of samples)
%
% nepe, Feb 2021
% Copyright (C) 2021 Chair of Automation Technology / TU Chemnitz

    gd = gpuDevice(); 
    data_in = gpuArray(data_in); 
    assert(isa(gather(data_in(1)), 'single')); % test if single precision input

    % devide data set into smaller pieces if it is to big for GPU
    iterations = ceil(size(data_in,1)/2000);
    output_complete = [];
    
    for it =1:iterations
        data = data_in((it-1)*2000+1:min(end,it*2000),:,:);
        seqL = size(data,2);

        % exploit double entries
        % the assumption is that there is a 50% overlap between sequences
        seqL = floor(seqL/2);
        data_orig = data;
        data = data(:,1:seqL,:);
        % add last half sequence 
        data(end+1,:,:) = data_orig(end,seqL+1:end,:);

        bs = size(data,1); 

        % used the same HD vector generated by the VSA_toolbox in script
        % 'preprocessing_context.m'
        Lo = load('data/oldInitSingle.mat');
        init_vector = frac_scale*reshape(Lo.init_vector(1:dim), 1, 1, dim);
        for i=1:9
            sensor_enc_vec(i,1,:) = reshape(Lo.sensor_enc_vec(1:dim,i), 1, 1, dim);
        end
        for i=1:64
            timestamps_vecs(1,i,:) = reshape(Lo.timestamps_vecs(1:dim,i), 1, 1, dim);
        end
        timestamps_mat = repmat( timestamps_vecs, bs, 1, 1 );

        % allocate matrices
        E = gpuArray(zeros(bs, seqL, dim, 'single'));
        SE = gpuArray(zeros(bs, seqL, dim, 'single'));

        init_vector = gpuArray(init_vector);
        sensor_enc_vec = gpuArray(sensor_enc_vec);
        timestamps_mat1 = gpuArray(timestamps_mat(1:end-1,1:seqL,:));
        timestamps_mat2 = gpuArray(timestamps_mat(1:end-1,seqL+1:end,:));

        BSE_cos_sum = gpuArray(zeros(bs, seqL, dim, 'single'));    
        BSE_sin_sum = gpuArray(zeros(bs, seqL, dim, 'single')); 

        % start runtime measure
        % everything before this point could be done once per lifetime of the algorithm
        totalTime = tic();

        % Encode data for each sensor type and accumulate
        % This Matlab loop does not hurt since it has only few iterations (e.g. 9 for UAH dataset)        
        for sensorID = 1:size(data,3)
            X = data(:,:,sensorID);        

            % encode using fractional binding
            % wrapping to +/- pi is not required since 
            E = X .* init_vector; % expands to a result of size bs x seqL x dim, each channel is one HDC dimension

            % bind to sensor ID
            SE = E + sensor_enc_vec(sensorID,1,:); % again, this automatically expands the vector sensor_enc_vec

            % accumulate                
            if i==1 % this is (IMHO) a fast way to reinitialize the large arrarys
                BSE_cos_sum = cos(SE);                
                BSE_sin_sum = sin(SE);
            else
                BSE_cos_sum = BSE_cos_sum + cos(SE);                
                BSE_sin_sum = BSE_sin_sum + sin(SE);
            end
        end

        % get angle
        BSE = atan2(BSE_sin_sum, BSE_cos_sum); % again of size bs x seqL x dim        
        clear BSE_sin_sum, BSE_cos_sum;

        %alternative approach: before binding, concatenate data
        %BSE = cat(2, BSE(1:end-1,:,:), BSE(2:end,:,:)); 

        % bind to time ID
        TBSE1 = BSE(1:end-1,:,:) + timestamps_mat1;
        TBSE2 = BSE(2:end,:,:) + timestamps_mat2;   
        clear BSE;

        % bundle over time
        TBSE_cos1 = cos(TBSE1);
        TBSE_sin1 = sin(TBSE1);
        TBSE_cos2 = cos(TBSE2);
        TBSE_sin2 = sin(TBSE2);

        TBSE_cos_sum1 = sum(TBSE_cos1, 2);
        TBSE_sin_sum1 = sum(TBSE_sin1, 2);
        TBSE_cos_sum2 = sum(TBSE_cos2, 2);
        TBSE_sin_sum2 = sum(TBSE_sin2, 2);

        TBSE_cos_sum = TBSE_cos_sum1 + TBSE_cos_sum2;
        TBSE_sin_sum = TBSE_sin_sum1 + TBSE_sin_sum2;

        STBSE = atan2(TBSE_sin_sum, TBSE_cos_sum);

        % finish
        output = reshape(STBSE, size(STBSE,1), size(STBSE,3));

        wait(gd); % IMPORTANT: wait for the GPU to finish! Alternatively, convert the outpout to CPU arary with gather(output)
        totalTime = toc(totalTime);

        output_complete = [output_complete; gather(output)];
        
        fprintf('Runtime is %f  (%f ms per sequence)\n', totalTime, totalTime*1000/size(data,1)); 
    end
end
   